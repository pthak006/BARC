import pandas as pd # Added for DataFrame manipulation
from tqdm import tqdm
import orjsonl
import os
from collections import Counter, defaultdict # Added defaultdict
# from datasets import load_dataset  # If you're not using this, can remove
from arc import validation_problems, ArcProblem # Added ArcProblem potentially needed? Check imports from original script if needed.
import argparse # Added to handle file paths arguments


# --- Constants and Configuration ---

# Only evaluate induction solutions.
MAX_FILES_TO_LOAD = 10000 # From original script

# POINT THIS TO YOUR DIRECTORY WITH THE .jsonl EXECUTION RESULT FILE(S):
# This should contain the files generated by eval_code_samples.py
INDUCTION_SAMPLE_EXEC_RESULTS_DIRS_AND_SAMPLE_SIZE = [
    ("results", 2048),  # e.g. "results" is the folder containing your single .jsonl, 128 samples per problem
                      # ADJUST THIS FOLDER AND SAMPLE SIZE
]

# Path to the metrics CSV file to load and update
# Use argparse for better flexibility
# DEFAULT_METRICS_CSV = 'metrics.csv' # Or 'augmented_metrics.csv' if that's the relevant one
DEFAULT_METRICS_CSV = 'augmented_metrics.csv' # Using the augmented one as per recent context

# Path to save the updated metrics CSV
DEFAULT_OUTPUT_CSV = 'metrics_eval_updated.csv'

# Column names in the CSV (ensure these match your file)
CSV_UID_COL = 'uid'
CSV_INDEX_COL = 'response_index'
NEW_EVAL_COL = 'is_correct_eval' # Name of the new column

# --- Helper Functions (from original script) ---

def grid_2d_to_tuple(grid):
    # Added error handling for non-list/array inputs (e.g., error strings)
    if not isinstance(grid, (list, tuple)) and not hasattr(grid, 'tolist'):
         # Handle cases like "error", "timeout" strings or other non-grid types
         # Return a unique non-grid tuple or handle as needed. Using input directly if string.
         return grid if isinstance(grid, str) else ("InvalidGridType",)
    try:
        # Attempt conversion assuming list-of-lists or numpy array
        return tuple(tuple(row) for row in grid)
    except TypeError:
        # Handle cases where elements might not be iterable (e.g., nested lists aren't uniform)
        return ("MalformedGrid",)


def tuple_to_grid_2d(t):
    # Add check if t is actually a grid tuple before converting
    if not isinstance(t, tuple) or not all(isinstance(row, tuple) for row in t):
         # Handle cases like ("InvalidGridType",) or error strings
         return t if isinstance(t, str) else [["ErrorConvertingTuple"]] # Or return the original non-tuple
    return [list(row) for row in t]

# --- Main Logic ---

def main(metrics_csv_path, output_csv_path, results_dir=None): # Pass paths as arguments
    # Override results directory if provided
    if results_dir:
        global INDUCTION_SAMPLE_EXEC_RESULTS_DIRS_AND_SAMPLE_SIZE
        INDUCTION_SAMPLE_EXEC_RESULTS_DIRS_AND_SAMPLE_SIZE = [(results_dir, 2048)]
        print(f"Using results directory: {results_dir}")
    
    print(f"Loading base metrics from: {metrics_csv_path}")
    try:
        df_metrics = pd.read_csv(metrics_csv_path, low_memory=False)
        print(f"Loaded {len(df_metrics)} rows.")
        # Initialize the new column to False (boolean)
        df_metrics[NEW_EVAL_COL] = False
        # Ensure column types are correct for later indexing/mapping
        df_metrics[CSV_UID_COL] = df_metrics[CSV_UID_COL].astype(str)
        # Coerce index col to numeric, then Int64 (nullable integer)
        df_metrics[CSV_INDEX_COL] = pd.to_numeric(df_metrics[CSV_INDEX_COL], errors='coerce')
        df_metrics.dropna(subset=[CSV_INDEX_COL], inplace=True) # Drop rows where index became NaN
        df_metrics[CSV_INDEX_COL] = df_metrics[CSV_INDEX_COL].astype(int)

    except FileNotFoundError:
        print(f"ERROR: Metrics CSV file not found at '{metrics_csv_path}'")
        return
    except Exception as e:
        print(f"ERROR: Failed to load or prepare metrics CSV: {e}")
        return

    uid_to_problem = {p.uid: p for p in validation_problems}

    # --- Step 1 & 2: Gather and Merge Execution Results (mostly same as original) ---
    # (Assuming only one source directory for simplicity in this example,
    # the logic remains the same if multiple sources are needed)

    merged_data = {} # Store merged results keyed by uid
    jsonl_load_errors = 0
    total_jsonl_files_loaded = 0

    print("\n--- Loading Execution Results (JSONL) ---")
    for induction_dir, num_samples_used in INDUCTION_SAMPLE_EXEC_RESULTS_DIRS_AND_SAMPLE_SIZE:
        if not os.path.isdir(induction_dir):
             print(f"Warning: Directory not found: {induction_dir}. Skipping.")
             continue

        try:
            jsonl_files = [f for f in os.listdir(induction_dir) if f.endswith(".jsonl")]
            jsonl_files.sort()
            jsonl_files = jsonl_files[:MAX_FILES_TO_LOAD]
            total_jsonl_files_loaded += len(jsonl_files)
        except FileNotFoundError:
             print(f"Warning: Error listing files in directory: {induction_dir}. Skipping.")
             continue

        if not jsonl_files:
            print(f"Warning: No .jsonl files found in {induction_dir}.")
            continue

        print(f"Loading {len(jsonl_files)} jsonl files from {induction_dir} (using max {num_samples_used} samples per problem)...")
        all_data_chunk = []
        for file in tqdm(jsonl_files, desc=f"Loading {induction_dir}"):
            path = os.path.join(induction_dir, file)
            try:
                # Using orjsonl which loads the whole file at once
                all_data_chunk.extend(orjsonl.load(path=path))
            except Exception as e:
                 print(f"Warning: Failed to load or parse {path}: {e}")
                 jsonl_load_errors += 1

        # Process loaded data into merged_data dict
        for problem_dict in tqdm(all_data_chunk, desc="Processing loaded problems"):
             try:
                 uid = str(problem_dict["uid"]) # Ensure UID is string
                 # Basic check for required keys
                 if "train_verdicts" not in problem_dict or "output_grids" not in problem_dict:
                     # print(f"Warning: Missing 'train_verdicts' or 'output_grids' for uid {uid}. Skipping entry.")
                     jsonl_load_errors += 1
                     continue

                 if uid not in merged_data:
                     merged_data[uid] = {"train_verdicts": [], "output_grids": [], "response_indices_map": []} # Store original indices if needed

                 # Add data, respecting num_samples_used limit *per problem*
                 current_len = len(merged_data[uid]["train_verdicts"])
                 needed = num_samples_used - current_len
                 if needed > 0:
                     take_n = min(needed, len(problem_dict["train_verdicts"]))
                     merged_data[uid]["train_verdicts"].extend(problem_dict["train_verdicts"][:take_n])
                     merged_data[uid]["output_grids"].extend(problem_dict["output_grids"][:take_n])
                     # If original index mapping is needed: merged_data[uid]["response_indices_map"].extend(list(range(take_n)))

             except KeyError as e:
                 # print(f"Warning: Missing key {e} in problem dict. Skipping entry.")
                 jsonl_load_errors += 1
             except Exception as e:
                 # print(f"Warning: Error processing entry for uid {uid}: {e}")
                 jsonl_load_errors += 1


    # Final check and trim after merging (if multiple files contributed to one UID)
    for uid, vals in merged_data.items():
        vals["train_verdicts"] = vals["train_verdicts"][:num_samples_used]
        vals["output_grids"]   = vals["output_grids"][:num_samples_used]
        # Ensure consistency after potential merges and trimming
        n_verdicts = len(vals["train_verdicts"])
        n_outputs  = len(vals["output_grids"])
        if n_verdicts != n_outputs:
             print(f"CRITICAL WARNING: Mismatch in final counts for {uid} ({n_verdicts} vs {n_outputs}). Data might be corrupt.")
             # Decide how to handle - skip UID? try to fix? For now, maybe trim to min length
             min_len = min(n_verdicts, n_outputs)
             vals["train_verdicts"] = vals["train_verdicts"][:min_len]
             vals["output_grids"]   = vals["output_grids"][:min_len]
             print(f"Attempted fix: Trimmed {uid} data to length {min_len}.")


    if jsonl_load_errors > 0:
         print(f"WARNING: Encountered {jsonl_load_errors} errors/skipped lines/entries during JSONL loading/processing.")
    if not merged_data:
         print("ERROR: No valid execution data loaded from JSONL files. Cannot proceed.")
         return
    print(f"Finished loading and merging execution data for {len(merged_data)} problems.")


    # --- Step 3: Perform Pass@2 Eval & Track Contributing Samples ---
    print("\n--- Performing Pass@2 Evaluation and Tracking Contributors ---")
    # Dictionary to store results: key=(uid_str, response_index), value=bool
    is_correct_eval_dict = defaultdict(bool)

    for uid, vals in tqdm(merged_data.items(), desc="Evaluating problems"):
        if uid not in uid_to_problem:
             # print(f"Warning: UID {uid} found in results but not in validation_problems set. Skipping.")
             continue # Skip if problem definition not found

        problem = uid_to_problem[uid]
        num_test  = len(problem.test_pairs)
        num_train = len(problem.train_pairs)
        num_samples_for_uid = len(vals["train_verdicts"]) # Actual number of samples processed for this UID

        # Process each test index for this problem
        for test_idx in range(num_test):
            gt_grid_list = problem.test_pairs[test_idx].y.tolist() # Ground truth as list-of-lists

            # Tally outputs for this test_idx from training-passing samples
            counter = Counter()
            training_pass_indices = [] # Store indices of samples that passed training
            for sample_idx in range(num_samples_for_uid):
                if vals["train_verdicts"][sample_idx]: # Check if this sample passed training
                    training_pass_indices.append(sample_idx)
                    # Get the output grid for this test_idx produced by this training-passing sample
                    # Check if output grid list is long enough
                    if len(vals["output_grids"][sample_idx]) > (num_train + test_idx):
                         output_grid = vals["output_grids"][sample_idx][num_train + test_idx]
                         test_grid_tuple = grid_2d_to_tuple(output_grid) # Convert to tuple for hashing
                         # Skip counting non-grid results like "error"
                         if isinstance(test_grid_tuple, tuple) and test_grid_tuple not in [("InvalidGridType",), ("MalformedGrid",)]:
                              counter[test_grid_tuple] += 1
                    # else: print(f"Warning: Output grid list too short for uid {uid}, sample {sample_idx}, test {test_idx}")


            # Find top 2 most frequent grid tuples and check if any match ground truth
            top_2_tuples = counter.most_common(2)
            pass2_success_for_this_test = False
            correct_top_tuples = set() # Store the tuple(s) from top_2 that ARE correct

            if not top_2_tuples: # Handle case where no training-passing samples produced valid output
                 continue

            for grid_tuple, _ in top_2_tuples:
                 # Check if grid_tuple represents a valid grid before converting
                 if isinstance(grid_tuple, tuple) and grid_tuple not in [("InvalidGridType",), ("MalformedGrid",)]:
                      candidate_grid_list = tuple_to_grid_2d(grid_tuple)
                      if candidate_grid_list == gt_grid_list:
                           pass2_success_for_this_test = True
                           correct_top_tuples.add(grid_tuple)
                 # else: This was likely an error string or invalid grid placeholder


            # If Pass@2 succeeded for this test case, mark the contributing samples
            if pass2_success_for_this_test:
                 for sample_idx in training_pass_indices: # Only check samples that passed training
                     # Check if output grid list is long enough
                     if len(vals["output_grids"][sample_idx]) > (num_train + test_idx):
                         output_grid = vals["output_grids"][sample_idx][num_train + test_idx]
                         sample_output_tuple = grid_2d_to_tuple(output_grid)
                         # Check if this sample's output was one of the CORRECT top tuples
                         if sample_output_tuple in correct_top_tuples:
                             # Mark this sample as True in our dictionary
                             # It contributed positively for at least one test case
                             is_correct_eval_dict[(uid, sample_idx)] = True
                     # else: Mismatch already warned about or handled


    # --- Step 4: Update the DataFrame ---
    print("\n--- Updating DataFrame ---")
    # Create a boolean Series based on the dictionary results, aligned with DataFrame's MultiIndex
    # Create the multi-index tuples from the DataFrame
    df_index_tuples = [idx for idx in df_metrics.set_index([CSV_UID_COL, CSV_INDEX_COL]).index]

    # Map the dictionary values to the DataFrame index
    # Use .get(idx, False) to default to False if a (uid, index) pair wasn't marked True
    update_series = pd.Series([is_correct_eval_dict.get(idx, False) for idx in df_index_tuples], index=df_metrics.index)

    df_metrics[NEW_EVAL_COL] = update_series
    print(f"Updated '{NEW_EVAL_COL}' column.")
    # Verify update (optional)
    print(f"Number of entries marked True in '{NEW_EVAL_COL}': {df_metrics[NEW_EVAL_COL].sum()}")


    # --- Step 5: Save the Updated DataFrame ---
    print(f"\n--- Saving Updated Metrics ---")
    try:
        df_metrics.to_csv(output_csv_path, index=False)
        print(f"Successfully saved updated metrics to: {output_csv_path}")
    except Exception as e:
        print(f"ERROR: Failed to save updated metrics CSV: {e}")


    # --- Step 6: Calculate and Print Overall Pass@2 Score (optional, same as original) ---
    print("\n--- Calculating Overall Pass@2 Score (for verification) ---")
    # Recalculate Pass@2 score using the same logic to ensure consistency
    # (This part is mostly copied from the original script for verification)
    induction_pass_at_2 = 0.0
    num_tasks_evaluated = 0
    for uid, vals in merged_data.items():
        if uid not in uid_to_problem: continue # Skip if problem not in validation set

        num_tasks_evaluated += 1
        problem = uid_to_problem[uid]
        num_test = len(problem.test_pairs)
        num_train = len(problem.train_pairs)
        num_samples_for_uid = len(vals["train_verdicts"])
        task_passed = True # Assume task passes unless a test pair fails

        for test_idx in range(num_test):
            gt_grid_list = problem.test_pairs[test_idx].y.tolist()
            counter = Counter()
            for sample_idx in range(num_samples_for_uid):
                 if vals["train_verdicts"][sample_idx]:
                      if len(vals["output_grids"][sample_idx]) > (num_train + test_idx):
                           output_grid = vals["output_grids"][sample_idx][num_train + test_idx]
                           test_grid_tuple = grid_2d_to_tuple(output_grid)
                           if isinstance(test_grid_tuple, tuple) and test_grid_tuple not in [("InvalidGridType",), ("MalformedGrid",)]:
                               counter[test_grid_tuple] += 1

            top_2_tuples = counter.most_common(2)
            test_pair_passed = False
            for grid_tuple, _ in top_2_tuples:
                 if isinstance(grid_tuple, tuple) and grid_tuple not in [("InvalidGridType",), ("MalformedGrid",)]:
                      if tuple_to_grid_2d(grid_tuple) == gt_grid_list:
                           test_pair_passed = True
                           break # Found a match in top 2

            if not test_pair_passed:
                 task_passed = False # If any test pair fails, the task fails Pass@2 for simple counting
                 # break # For original partial credit logic, remove break and use += below
            else:
                 # Original logic gave partial credit per test pair
                 induction_pass_at_2 += 1.0 / len(problem.test_pairs)


    # total # of validation tasks
    num_tasks_total = len(validation_problems)
    print(f"Total Validation Problems: {num_tasks_total}")
    print(f"Problems Evaluated (found in JSONL results): {num_tasks_evaluated}")
    print(f"Calculated Induction pass@2 (partial credit): {induction_pass_at_2:.4f}")
    if num_tasks_evaluated > 0:
         print(f"Pass@2 Score (based on evaluated tasks): {induction_pass_at_2 / num_tasks_evaluated:.4f}")
    else:
         print("Pass@2 Score: N/A (no tasks evaluated)")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate ARC results using Pass@2 and update metrics CSV.")
    parser.add_argument(
        "--metrics_csv",
        default=DEFAULT_METRICS_CSV,
        help=f"Path to the input metrics CSV file (default: {DEFAULT_METRICS_CSV})"
    )
    parser.add_argument(
        "--output_csv",
        default=DEFAULT_OUTPUT_CSV,
        help=f"Path to save the updated metrics CSV file (default: {DEFAULT_OUTPUT_CSV})"
    )
    # Add arguments for result dirs
    parser.add_argument("--results_dir", default=None, help="Directory containing *_exec_results_v4.jsonl files")

    args = parser.parse_args()
    main(args.metrics_csv, args.output_csv, args.results_dir)